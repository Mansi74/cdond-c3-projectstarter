version: 2.1

commands:
    install_awscli:
      description: Install aws cli
      steps:
        - run:
            name: Install aws cli
            command: |
              curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
              unzip awscliv2.zip
              sudo ./aws/install

    install_ansible:
      description: install ansible
      steps:
        - run:
            name: install ansible
            command: |
              sudo apt update
              sudo apt install software-properties-common
              sudo apt-add-repository --yes --update ppa:ansible/ansible
              sudo apt install ansible

    destroy-environment:
      description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
      parameters:
        Workflow_ID:
          type: string
          default: ${CIRCLE_WORKFLOW_ID:0:7}
      steps:
        - run:
            name: Destroy environments
            when: on_fail
            command: |
              aws cloudformation delete-stack --stack-name udapeople-backend-<< parameters.Workflow_ID >>
              aws s3 rm s3://udapeople-<< parameters.Workflow_ID >> --recursive
              aws cloudformation delete-stack --stack-name udapeople-frontend-<< parameters.Workflow_ID >>

    revert-migrations:
      description: Revert the last migration if successfully run in the current workflow.
      parameters:
        Workflow_ID:
          type: string
          default: ${CIRCLE_WORKFLOW_ID:0:7}
      steps:
        - run:
            name: Revert migrations
            when: on_fail
            command: |
              SUCCESS=$(curl --insecure https://kvdb.io/${KVDB_BUCKET}/migration_<< parameters.Workflow_ID >>)
              # Logic for reverting the database state
              if(($SUCCESS == 1 ));
              then 
                cd ~/project/backend
                npm install
                npm run migration:revert
              fi
              
jobs:
  build-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Build front-end
          command: |
            cd frontend
            npm install
            npm run build
      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-deps


  build-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end build
          command: |
             cd backend
             npm install
             npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-deps


  test-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end Unit Test
          command: |
            cd frontend
            npm install
            npm test



                
  test-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Back-end Unit Test
          command: |
            cd backend
            npm install
            npm test
   

  scan-frontend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-deps]
      - run:
          name: Front-end scan
          command: |
            cd frontend
            npm install
            npm audit fix --force --audit-level=critical
            npm audit --audit-level=critical
  


  scan-backend:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-deps]
      - run:
          name: Back-end scan
          command: |
            cd backend
            npm install
            npm audit fix --force --audit-level=critical
            npm audit fix --force --audit-level=critical
            npm audit --audit-level=critical


  deploy-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople \
              --stack-name "udapeople-backend-cli-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=udapeople \
              --stack-name "udapeople-frontend-cli-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
              
      - run:
          name: Add back-end ip to ansible inventory
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --query 'Reservations[*].Instances[*].PublicIpAddress' \
              --output text)
            echo $BACKEND_PUBLIC_IP Â» .circleci/ansible/inventory.txt
            cat .circleci/ansible/inventory.txt
            
      - persist_to_workspace:
          root: ~/
          paths:
            - project/.circleci/ansible/inventory.txt

      - destroy-environment
      # Here's where you will add some code to rollback on failure      

  configure-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - install_ansible
      - add_ssh_keys:
          fingerprints: ["30:25:02:0f:4a:0c:1e:da:93:42:fb:dc:5e:21:c6:6b"]
      - attach_workspace:
          at: ~/
        
      - run:
          name: Configure server
          command: |
              cd .circleci/ansible
              cat inventory.txt
              ansible-playbook -i inventory.txt configure-server.yml
      - destroy-environment
        # Here's where you will add some code to rollback on failure      

  run-migrations:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - install_awscli
      - run:
          name: Run migrations
          command: |
            cd backend
            npm install
            npm run migrations > migrations_dump.txt
      - run:
          name: Send migration status to kvdb.io
          command: |
            if grep -q "has been executed successfully." ~/project/backend/migrations_dump.txt
            then
              curl https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7} -d '1'
            fi
      - destroy-environment
      - revert-migrations
    # Here's where you will add some code to rollback on failure      

  deploy-frontend:
      docker:
        - image: sleavely/node-awscli:14.x
      steps:
        - checkout
        - attach_workspace:
            at: /root
        - run:
            name: Get backend url
            command: |
              API_URL=$(curl -H "Content-Type: text/plain" \
              -H "token: ecdfc255-81e1-4056-9efa-2d7fa10812e1" \
              --request GET \
              https://api.memstash.io/values/backend_url)
              echo $API_URL
              echo "${API_URL}"
              echo "NODE_ENV=production" >> frontend/.env
              echo "API_URL=${API_URL}" >> frontend/.env
              cat frontend/.env
        - run:
            name: Deploy frontend objects
            working_directory: ./frontend
            command: |
              npm i
              npm run build
              ls -l
              aws s3 cp ./dist "s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7}" --recursive
        # Here's where you will add some code to rollback on failure
        - destroy-environment:
            workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}
            

    deploy-backend:
      docker:
        - image: python:3.7-alpine3.11
          environment:
            NODE_ENV: "local"
            VERSION: "1"
            ENVIRONMENT: "production"
            TYPEORM_CONNECTION: $TYPEORM_CONNECTION
            TYPEORM_HOST: $TYPEORM_HOST
            TYPEORM_USERNAME: $TYPEORM_USERNAME
            TYPEORM_PASSWORD: $TYPEORM_PASSWORD
            TYPEORM_DATABASE: $TYPEORM_DATABASE
            TYPEORM_PORT: $TYPEORM_PORT
            TYPEORM_ENTITIES: $TYPEORM_ENTITIES
      steps:
        - checkout
        - add_ssh_keys:
            fingerprints: ["eb:8c:c3:a9:4b:36:3f:c7:69:df:e2:36:d3:22:f5:ed"]
        - attach_workspace:
            at: /root
        - run:
            name: Install dependencies
            command: |
              apk add --update ansible rsync 
              apk add curl
              pip3 install awscli
        - run:
            name: Deploy backend
            command: |
              ls /root
              ls /root/project/backend/
              cat ~/project/.circleci/ansible/inventory.txt
              ansible-playbook -i ~/project/.circleci/ansible/inventory.txt ~/project/.circleci/ansible/deploy-backend.yml
        # Here's where you will add some code to rollback on failure
        - destroy-environment:
            workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}

    smoke-test:
      docker:
        - image: python:3.7-alpine3.11
      steps:
        - checkout
        - attach_workspace:
            at: /root
        - run:
            name: Install dependencies
            command: |
              apk add --update curl
              pip install awscli
        - run:
            name: Backend smoke test.
            command: |
              ls 
              pwd
              export backend_url=$(curl -H "token: ecdfc255-81e1-4056-9efa-2d7fa10812e1" \
              --request GET https://api.memstash.io/values/backend_url)
              echo $backend_url
              # curl "$backend_url/api/status"
        - run:
            name: Frontend smoke test.
            command: |
              URL="http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website-us-east-1.amazonaws.com"
              if curl -s ${URL} | grep "Welcome"
              then
                return 0
              else
                return 1
              fi
        - destroy-environment:
            workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}
        - revert-migrations:
            workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}

    cloudfront-update:
      docker:
        - image: amazon/aws-cli
      steps:
        - checkout
        - run:
            name: get old workflow for clean
            command: |
              WorkflowID=$(aws cloudformation \
              list-exports --query "Exports[?Name==\`WorkflowID\`].Value" \
              --no-paginate --output text)
              curl -H "Content-Type: text/plain" -H "token: ecdfc255-81e1-4056-9efa-2d7fa10812e1" --request PUT --data "$WorkflowID" https://api.memstash.io/values/workflowid
        - run:
            name: Update cloudfront distribution
            command: |
              aws cloudformation deploy \
                --template-file .circleci/files/cloudfront.yml \
                --tags project=circleci-${CIRCLE_WORKFLOW_ID:0:7} \
                --stack-name "udapeople-cloudfrontstack" \
                --parameter-overrides WorkflowID="${CIRCLE_WORKFLOW_ID:0:7}"
        # Here's where you will add some code to rollback on failure
        - revert-migrations:
            workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}
        - destroy-environment:
            workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}

    cleanup:
      docker:
        - image: amazon/aws-cli
      steps:
        - checkout
        - run:
            name: Remove old stacks and files
            command: |
              OldWorkflowID=$(curl -H "token: ecdfc255-81e1-4056-9efa-2d7fa10812e1" --request GET https://api.memstash.io/values/workflowid)
              CurrentWorkflowID=${CIRCLE_WORKFLOW_ID:0:7}
              if [[ $OldWorkflowID == "" ]]
              then
              echo "No workflow id found"
              else
              if [[ "$CurrentWorkflowID" != "$OldWorkflowID" ]]
              then
                echo "$OldWorkflowID!=$CurrentWorkflowID => will delete old version"
                aws s3 rm "s3://udapeople-${OldWorkflowID}" --recursive
                aws cloudformation delete-stack --stack-name "udapeople-frontend-${OldWorkflowID}"
                aws cloudformation delete-stack --stack-name "udapeople-backend-${OldWorkflowID}"
              fi
              fi

            
  

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - scan-backend:
          requires: [build-backend]
      - scan-frontend:
          requires: [build-frontend]
      - deploy-infrastructure:
          requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          filters:
            branches:
              only: [master]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - run-migrations:
          requires: [configure-infrastructure]
      - deploy-frontend:
          requires: [run-migrations]
      - deploy-backend:
          requires: [run-migrations]
      - smoke-test:
          requires: [deploy-backend, deploy-frontend]
      - cloudfront-update:
          requires: [smoke-test]
      - cleanup:
          requires: [cloudfront-update]